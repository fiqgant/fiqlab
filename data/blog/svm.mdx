---
title: ðŸ§® Support Vector Machine (SVM)
date: '2022-08-08'
tags: ['Algorithm', 'SVM', 'ML']
draft: false
summary: Support Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier that best segregates the two classes (hyper-plane/ line).
---

<div className="grid place-items-center">
  <img
    className="inline rounded-lg"
    src="https://patrastatistika.com/wp-content/uploads/2021/11/gambar-SVM.png"
    alt="SVM"
    width="700"
    height="700"
  />
</div>

# Introduction
Consider machine learning algorithms to be an armory full of axes, swords, blades, arrows, daggers, and so on. You have a variety of tools, but you must learn when to use them. Consider 'Regression' to be a sword capable of efficiently slicing and dicing data but incapable of dealing with very complex data. On the contrary, 'Support Vector Machines' are like a sharp knife: they function well with little datasets but can be considerably stronger and more powerful when developing machine learning models with large ones.
The "Support Vector Machine" (SVM) is a supervised machine learning technique that can be used for classification and regression tasks. It is, however, mostly used in categorization difficulties. Each data item is plotted as a point in $n$-dimensional space (where n is the number of features you have), with the value of each feature being the value of a certain coordinate in the SVM algorithm. Then, we accomplish classification by locating the hyperplane that best distinguishes the two classes (look at the below snapshot).

<div className="grid place-items-center">
  <img
    className="inline rounded-lg"
    src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_1.png"
    alt="SVM2"
    width="700"
    height="700"
  />
</div>

Support Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier that best segregates the two classes (hyper-plane/ line).

# The Fundamentals of SVM
The concept of Support Vector Machine is simple: Given observations $x_i \in {\rm I\!R}^p$ and each observation $x_i$ has a class $y_i$, where $y_i \in \{-1,1\}$, we want to discover a hyperplane that can divide the observations based on their classes while simultaneously maximizing the observation's shortest distance to the hyperplane.
To begin, we define the best hyperplane as $w^Tx+b = 0$. From the linear algebra, we know the distance from a point $x_0$ to the plane $w^Tx+b = 0$ is calculated by:

$$
\begin{aligned} \text{Distance} = \dfrac{|w^Tx_0 + b|}{||w||} \end{aligned}
$$

Then we want such a hyperplane to be able to accurately distinguish two classes, which is equivalent to satisfying the following constraints:

$$
\begin{aligned} &(w^Tx_i+b) > 0, & & \text{if $y_i$ = 1} \\ &(w^Tx_i+b) < 0, & & \text{if $y_i$ = -1}, \end{aligned}
$$

which is equivalent to

$$
\begin{aligned} &y_i(w^Tx_i+b) > 0, & &i = 1, \ldots, n. \end{aligned}
$$

Our goal is to maximize the minimum distance for all observations to that hyperplane,so we have our first optimization problem:

$$
\begin{aligned} &\underset{w,b}{\text{maximize}} & & \text{min} \{\frac{|w^Tx_i+b|}{||w||}\}\\ & \text{s.t.} & & y_i(w^Tx_i+b) > 0,i = 1, \ldots, n. \end{aligned}
$$