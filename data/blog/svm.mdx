---
title: ðŸ§® Support Vector Machine (SVM)
date: '2022-08-08'
tags: ['Algorithm', 'SVM', 'ML']
draft: false
summary: Support Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier that best segregates the two classes (hyper-plane/ line).
---

<div className="grid place-items-center">
  <img
    className="inline rounded-lg"
    src="https://patrastatistika.com/wp-content/uploads/2021/11/gambar-SVM.png"
    alt="SVM"
    width="700"
    height="700"
  />
</div>

# Introduction
Consider machine learning algorithms to be an armory full of axes, swords, blades, arrows, daggers, and so on. You have a variety of tools, but you must learn when to use them. Consider 'Regression' to be a sword capable of efficiently slicing and dicing data but incapable of dealing with very complex data. On the contrary, 'Support Vector Machines' are like a sharp knife: they function well with little datasets but can be considerably stronger and more powerful when developing machine learning models with large ones.
The "Support Vector Machine" (SVM) is a supervised machine learning technique that can be used for classification and regression tasks. It is, however, mostly used in categorization difficulties. Each data item is plotted as a point in $n$-dimensional space (where n is the number of features you have), with the value of each feature being the value of a certain coordinate in the SVM algorithm. Then, we accomplish classification by locating the hyperplane that best distinguishes the two classes (look at the below snapshot).

<div className="grid place-items-center">
  <img
    className="inline rounded-lg"
    src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_1.png"
    alt="SVM2"
    width="700"
    height="700"
  />
</div>

Support Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier that best segregates the two classes (hyper-plane/ line).

# The Fundamentals of SVM
The concept of Support Vector Machine is simple: Given observations $x_i \in {\rm I\!R}^p$ and each observation $x_i$ has a class $y_i$, where $y_i \in \{-1,1\}$, we want to discover a hyperplane that can divide the observations based on their classes while simultaneously maximizing the observation's shortest distance to the hyperplane.
To begin, we define the best hyperplane as $w^Tx+b = 0$. From the linear algebra, we know the distance from a point $x_0$ to the plane $w^Tx+b = 0$ is calculated by:

$$
\begin{aligned} \text{Distance} = \dfrac{|w^Tx_0 + b|}{||w||} \end{aligned}
$$

Then we want such a hyperplane to be able to accurately distinguish two classes, which is equivalent to satisfying the following constraints:

$$
\begin{aligned} &(w^Tx_i+b) > 0, & & \text{if $y_i$ = 1} \\ &(w^Tx_i+b) < 0, & & \text{if $y_i$ = -1}, \end{aligned}
$$

which is equivalent to

$$
\begin{aligned} &y_i(w^Tx_i+b) > 0, & &i = 1, \ldots, n. \end{aligned}
$$

Our goal is to maximize the minimum distance for all observations to that hyperplane,so we have our first optimization problem:

$$
\begin{aligned} &\underset{w,b}{\text{maximize}} & & \text{min} \{\frac{|w^Tx_i+b|}{||w||}\}\\ & \text{s.t.} & & y_i(w^Tx_i+b) > 0,i = 1, \ldots, n. \end{aligned}
$$

Then we define margin $M = \text{min}\{\frac{|w^Tx_i+b|}{||w||}\}$ and for the mathematics convenience, we scale $w$ such that satisfy $||w|| = 1$

$$
\begin{aligned} &\underset{w,||w|| = 1,b}{\text{maximize}} & & M\\ & \text{ s.t.} & & y_i(w^Tx_i+b) \geq M, i = 1, \ldots, n. \end{aligned}
$$


Then we define $v = \frac{w}{M}$ and the norm of the $||v||$ is therefore \frac{1}{M}. We substitute $v$ back to our optimization:

$$
\begin{aligned} &\underset{v,b}{\text{maximize}} & & \frac{1}{||v||}\\ & \text{s.t.} & & y_i(\frac{w^T}{M}x_i+\frac{b}{M}) \geq \frac{M}{M} = 1, i = 1, \ldots, n. \end{aligned}
$$

Then we change the variable name $v$ to $w$, and to maximize $\frac{1}{||v||}$ is equivalent to minimize $\frac{1}{2}w^Tw$. So we get our final optimization problem as:

$$
\begin{aligned} &\underset{w,b}{\text{minimize}} & & \frac{1}{2}w^Tw\\ & \text{s.t.} & & y_i(w^Tx_i+b) \geq 1, i = 1, \ldots, n. \end{aligned}
$$

We want to use the Method of Lagrange Multipliers to solve the optimization problem. We can write our constrained as $g_i(w) = y_i(w^T x_i+b) - 1 \geq 0$. Lets define $L(w,b,\alpha \geq 0) = \frac{1}{2}w^Tw - \sum_{i=1}^{n}\alpha_i[y_i(w^Twx_i+b) -1]$. We observe that the maximum of the function $L$ with respect to $\alpha$ equals \frac{1}{2}w^Tw. So we change our original optimization problem to the new one :

$$
\begin{aligned} &\underset{w,b}{\text{min }} \underset{\alpha \geq 0}{\text{max }}L(w,b,\alpha) = \frac{1}{2}w^Tw - \sum_{i=1}^{n}\alpha_i[y_i(w^Twx_i+b) -1] \end{aligned}
$$

where $\alpha$ is the Lagrange Multiplier. 
We can verify that $w,b,\alpha$ satisfy Karush-Kuhn-Tucker (KKT condition). Therefore, we can solve the primal problem by solving its dual problem:

$$
\begin{aligned} &\underset{\alpha \geq 0}{\text{max }}\underset{w,b}{\text{min }} L(w,b,\alpha) = \frac{1}{2}w^Tw - \sum_{i=1}^{n}\alpha_i[y_i(w^Twx_i+b) -1] \end{aligned}
$$
